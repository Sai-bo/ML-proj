{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd168d4",
   "metadata": {},
   "source": [
    "# DNN\n",
    "Our result is generated by this script. The first line of each block briefly explain the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65479534",
   "metadata": {},
   "source": [
    "### Install and import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a52fcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c48c9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db01d53",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6647343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vHDhZSPmhithLVRRzkNw0ak_kk7PhInu\n",
      "To: /workspace/V_trade.csv\n",
      "100%|█████████████████████████████████████████| 230M/230M [00:00<00:00, 240MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=145T8z3XXlsaISzWdrJgJVANGokG1XuFI\n",
      "To: /workspace/V_remit.csv\n",
      "100%|███████████████████████████████████████| 90.4M/90.4M [00:00<00:00, 149MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1AubrUmeNUgpgiOu4tay6Gwl8O3lBaokF\n",
      "To: /workspace/V_info.csv\n",
      "100%|████████████████████████████████████████| 557k/557k [00:00<00:00, 28.9MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1zZo9RLt3mMmJZxEETSY2g9ND31qkZIn0\n",
      "To: /workspace/V_cred.csv\n",
      "100%|███████████████████████████████████████| 12.3M/12.3M [00:00<00:00, 186MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1uFCx21bqE3FnrdfvN_mwtw2-nvogTEex\n",
      "To: /workspace/V_cons.csv\n",
      "100%|█████████████████████████████████████████| 105M/105M [00:00<00:00, 179MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ZOXGT_rIdEGIliHGKEH3ha77ZlZyq1Gn\n",
      "To: /workspace/train_y.csv\n",
      "100%|████████████████████████████████████████| 204k/204k [00:00<00:00, 15.7MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1qjEwmi97OWdshSNdgQj2ccXnoM4UvT25\n",
      "To: /workspace/V_trade_public.csv\n",
      "100%|███████████████████████████████████████| 19.0M/19.0M [00:00<00:00, 184MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1g8trBiC6OxuoTU94u_UMygVrA-fSASpB\n",
      "To: /workspace/V_remit_public.csv\n",
      "100%|███████████████████████████████████████| 7.35M/7.35M [00:00<00:00, 135MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=14KTfY56Mz2xBXdP27GvGB2HVeb_4Ks4T\n",
      "To: /workspace/V_info_public.csv\n",
      "100%|██████████████████████████████████████| 41.7k/41.7k [00:00<00:00, 24.5MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1EaIWnjQxUl4KRgVCqYT7AB4PaSNvc_GL\n",
      "To: /workspace/V_cred_public.csv\n",
      "100%|██████████████████████████████████████| 1.08M/1.08M [00:00<00:00, 37.8MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1owf1urxHZywAxJfCgVXCpXEMQ6VZGhnO\n",
      "To: /workspace/V_cons_public.csv\n",
      "100%|██████████████████████████████████████| 10.2M/10.2M [00:00<00:00, 91.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Download processed data\n",
    "\n",
    "!gdown 1vHDhZSPmhithLVRRzkNw0ak_kk7PhInu # V_trade\n",
    "!gdown 145T8z3XXlsaISzWdrJgJVANGokG1XuFI # V_remit\n",
    "!gdown 1AubrUmeNUgpgiOu4tay6Gwl8O3lBaokF # V_info\n",
    "!gdown 1zZo9RLt3mMmJZxEETSY2g9ND31qkZIn0 # V_cred\n",
    "!gdown 1uFCx21bqE3FnrdfvN_mwtw2-nvogTEex # V_cons\n",
    "!gdown 1ZOXGT_rIdEGIliHGKEH3ha77ZlZyq1Gn # train_y\n",
    "!gdown 1qjEwmi97OWdshSNdgQj2ccXnoM4UvT25 # V_trade_public\n",
    "!gdown 1g8trBiC6OxuoTU94u_UMygVrA-fSASpB # V_remit_public\n",
    "!gdown 14KTfY56Mz2xBXdP27GvGB2HVeb_4Ks4T # V_info_public\n",
    "!gdown 1EaIWnjQxUl4KRgVCqYT7AB4PaSNvc_GL # V_cred_public\n",
    "!gdown 1owf1urxHZywAxJfCgVXCpXEMQ6VZGhnO # V_cons_public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf319e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data as csv\n",
    "\n",
    "V_cons = pd.read_csv('V_cons.csv').iloc[:, 1:]\n",
    "V_cred = pd.read_csv('V_cred.csv').iloc[:, 1:]\n",
    "V_info = pd.read_csv('V_info.csv').iloc[:, 1:]\n",
    "V_remit = pd.read_csv('V_remit.csv').iloc[:, 1:]\n",
    "V_trade = pd.read_csv('V_trade.csv').iloc[:, 1:]\n",
    "train_y = pd.read_csv('train_y.csv').iloc[:, 1:]\n",
    "\n",
    "V_cons_public = pd.read_csv('V_cons_public.csv').iloc[:, 1:]\n",
    "V_cred_public = pd.read_csv('V_cred_public.csv').iloc[:, 1:]\n",
    "V_info_public = pd.read_csv('V_info_public.csv').iloc[:, 1:]\n",
    "V_remit_public = pd.read_csv('V_remit_public.csv').iloc[:, 1:]\n",
    "V_trade_public = pd.read_csv('V_trade_public.csv').iloc[:, 1:]\n",
    "\n",
    "public_x_alert_date = pd.read_csv('public_x_alert_date.csv')\n",
    "all_keys = pd.read_csv('all_keys.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee62d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate dataframes to get the entire training/testing data\n",
    "# some values are missing, fill them with 0\n",
    "\n",
    "V_overall = pd.concat([V_info, V_cred, V_cons, V_remit, V_trade], axis=1).fillna(0)\n",
    "V_overall_public = pd.concat([V_info_public, V_cred_public, V_cons_public, V_remit_public, V_trade_public], axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00fa34fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23906, 4)\n",
      "(23906, 117)\n",
      "(23906, 1965)\n",
      "(23906, 1572)\n",
      "(23906, 3537)\n",
      "(23906, 7195)\n",
      "\n",
      "(1845, 4)\n",
      "(1845, 117)\n",
      "(1845, 1965)\n",
      "(1845, 1572)\n",
      "(1845, 3537)\n",
      "(1845, 7195)\n",
      "\n",
      "(23906, 1)\n",
      "\n",
      "(1845, 2)\n",
      "(25751, 1)\n"
     ]
    }
   ],
   "source": [
    "# verify the shape of dataframes\n",
    "\n",
    "print(V_info.shape)\n",
    "print(V_cred.shape)\n",
    "print(V_cons.shape)\n",
    "print(V_remit.shape)\n",
    "print(V_trade.shape)\n",
    "print(V_overall.shape)\n",
    "print()\n",
    "print(V_info_public.shape)\n",
    "print(V_cred_public.shape)\n",
    "print(V_cons_public.shape)\n",
    "print(V_remit_public.shape)\n",
    "print(V_trade_public.shape)\n",
    "print(V_overall_public.shape)\n",
    "print()\n",
    "print(train_y.shape)\n",
    "print()\n",
    "print(public_x_alert_date.shape)\n",
    "print(all_keys.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afac17c",
   "metadata": {},
   "source": [
    "### Settings\n",
    "There are three choice of cost function. The best result is generated by \"linear\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68991903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "n_epoch = 33  # number of epochs\n",
    "batch = 128  # batch size\n",
    "lr = 0.0000001  # learning rate\n",
    "w = 1  # penalty weight for false negative\n",
    "d = 99  # duplicate d times for SAR_flag == 1 (oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd5b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.network = nn.Sequential( # 7195 -> 1\n",
    "            nn.Linear(7195, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid() # last one must be sigmoid \n",
    "        )\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73b71b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function (linear)\n",
    "\n",
    "def loss_function(prob, ans):\n",
    "    return (w * (1 - prob) * ans + (prob) * (1 - ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec767edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function (quadratic)\n",
    "\n",
    "# def loss_function(prob, ans):\n",
    "#     # a * x**n\n",
    "#     a = 2\n",
    "#     n = 2\n",
    "#     return (w * a * (1 - prob)**n * ans + a * (prob)**n * (1 - ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6977cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function (log)\n",
    "\n",
    "# prob = torch.minimum(prob, torch.full((prob.size(dim=0), 1), 0.9999999).to(device))\n",
    "# prob = torch.maximum(prob, torch.full((prob.size(dim=0), 1), 0.0000001).to(device))\n",
    "# return (w * torch.log(1 - prob) * ans + torch.log(prob) * (1 - ans))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a424a177",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43a8a5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3ad1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training process\n",
    "\n",
    "def train(train_data, val_data, model, n_epoch, batch, lr, device):\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    best_loss = 1000000\n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        idx = 0\n",
    "        for data, ans in train_data:\n",
    "            data, ans = data.to(device), ans.to(device)\n",
    "            prob = model(data)\n",
    "            loss = torch.sum(loss_function(prob, ans)) / batch\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += (loss.item() / len(train_data))\n",
    "            print('[Epoch %d | %d/%d] loss: %.4f' % ((epoch+1), idx*batch, len(train_data) * batch, loss.item()), end='\\r')\n",
    "            idx += 1\n",
    "        print(\"\\n  Training  | Loss:%.4f \" % total_loss)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        idx = 0 \n",
    "        with torch.no_grad():\n",
    "            for data, ans in val_data:\n",
    "                data, ans = data.to(device), ans.to(device)\n",
    "                prob = model(data)\n",
    "                loss = torch.sum(loss_function(prob, ans)) / batch\n",
    "                total_loss += (loss.item() / len(val_data))\n",
    "                idx += 1\n",
    "            print(\" Validation | Loss:%.4f \" % total_loss)\n",
    "            \n",
    "        torch.save(model.state_dict(), \"%s\" % \"model.pth\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93609199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the whole dataset\n",
    "# Oversampling: we duplicate the samples with SAR_flag=1 for d times\n",
    "# d=99 let the number of samples of flag=0 and flag=1 matches (SAR_rate = 0.5)\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # oversampling\n",
    "        V_overall_list = V_overall.values.tolist()\n",
    "        train_y_list = train_y.values.tolist()\n",
    "        l = len(train_y_list)\n",
    "        s = sum(sum(train_y_list,[]))\n",
    "        print(\"Before oversampling: total=:\", l, \"flag=0:\", l - s, \"flag=1:\", s, \"SAR_rate=\", s / l)        \n",
    "        for i in range(l):\n",
    "            if train_y_list[i][0] == 1:\n",
    "                V_overall_list.extend([V_overall_list[i] for j in range(d)])\n",
    "                train_y_list.extend([train_y_list[i] for j in range(d)])\n",
    "        l = len(train_y_list)\n",
    "        s = sum(sum(train_y_list,[]))\n",
    "        print(\"After oversampling: total=:\", l, \"flag=0:\", l - s, \"flag=1:\", s, \"SAR_rate=\", s / l)    \n",
    "        \n",
    "        self.X = torch.tensor(V_overall_list).to(torch.float32)\n",
    "        self.Y = torch.tensor(train_y_list).to(torch.float32)\n",
    "  \n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "114fac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define testing dataset\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.X = torch.tensor(V_overall_public.values).to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d37ed",
   "metadata": {},
   "source": [
    "Please note that, since we do oversampling with a huge multiplicity (d=99), if we split the whole data set into training set and validation set randomly, many samples will occur in both set, so the validation is biased. Therefore, we do not do validation here. \n",
    "\n",
    "However, we keep the validation part in the script anyway. In the block below, the validation set is set to the same as training set. You can just ignore it. During training process, we evaluate it but do not use it to determine when to stop the programm since the validation loss is meaningless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ba0c608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling: total=: 23906 flag=0: 23672 flag=1: 234 SAR_rate= 0.009788337655818623\n",
      "After oversampling: total=: 47072 flag=0: 23672 flag=1: 23400 SAR_rate= 0.4971108089734874\n"
     ]
    }
   ],
   "source": [
    "trainset = TrainDataset()\n",
    "train_dataloader = DataLoader(trainset, batch, True)\n",
    "val_dataloader = DataLoader(trainset, batch, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f226b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1 | 46976/47104] loss: 0.3203\n",
      "  Training  | Loss:0.4802 \n",
      " Validation | Loss:0.4789 \n",
      "[Epoch 2 | 46976/47104] loss: 0.3640\n",
      "  Training  | Loss:0.4775 \n",
      " Validation | Loss:0.4759 \n",
      "[Epoch 3 | 46976/47104] loss: 0.3712\n",
      "  Training  | Loss:0.4746 \n",
      " Validation | Loss:0.4728 \n",
      "[Epoch 4 | 46976/47104] loss: 0.2969\n",
      "  Training  | Loss:0.4712 \n",
      " Validation | Loss:0.4692 \n",
      "[Epoch 5 | 46976/47104] loss: 0.3868\n",
      "  Training  | Loss:0.4671 \n",
      " Validation | Loss:0.4635 \n",
      "[Epoch 6 | 46976/47104] loss: 0.3781\n",
      "  Training  | Loss:0.4618 \n",
      " Validation | Loss:0.4602 \n",
      "[Epoch 7 | 46976/47104] loss: 0.2877\n",
      "  Training  | Loss:0.4591 \n",
      " Validation | Loss:0.4578 \n",
      "[Epoch 8 | 46976/47104] loss: 0.2974\n",
      "  Training  | Loss:0.4567 \n",
      " Validation | Loss:0.4553 \n",
      "[Epoch 9 | 46976/47104] loss: 0.3886\n",
      "  Training  | Loss:0.4537 \n",
      " Validation | Loss:0.4521 \n",
      "[Epoch 10 | 46976/47104] loss: 0.3698\n",
      "  Training  | Loss:0.4506 \n",
      " Validation | Loss:0.4486 \n",
      "[Epoch 11 | 46976/47104] loss: 0.3312\n",
      "  Training  | Loss:0.4467 \n",
      " Validation | Loss:0.4446 \n",
      "[Epoch 12 | 46976/47104] loss: 0.3832\n",
      "  Training  | Loss:0.4428 \n",
      " Validation | Loss:0.4405 \n",
      "[Epoch 13 | 46976/47104] loss: 0.3616\n",
      "  Training  | Loss:0.4386 \n",
      " Validation | Loss:0.4362 \n",
      "[Epoch 14 | 46976/47104] loss: 0.2921\n",
      "  Training  | Loss:0.4343 \n",
      " Validation | Loss:0.4321 \n",
      "[Epoch 15 | 46976/47104] loss: 0.3652\n",
      "  Training  | Loss:0.4300 \n",
      " Validation | Loss:0.4275 \n",
      "[Epoch 16 | 46976/47104] loss: 0.3196\n",
      "  Training  | Loss:0.4252 \n",
      " Validation | Loss:0.4223 \n",
      "[Epoch 17 | 46976/47104] loss: 0.2898\n",
      "  Training  | Loss:0.4199 \n",
      " Validation | Loss:0.4168 \n",
      "[Epoch 18 | 46976/47104] loss: 0.2755\n",
      "  Training  | Loss:0.4144 \n",
      " Validation | Loss:0.4116 \n",
      "[Epoch 19 | 46976/47104] loss: 0.2996\n",
      "  Training  | Loss:0.4088 \n",
      " Validation | Loss:0.4062 \n",
      "[Epoch 20 | 46976/47104] loss: 0.2324\n",
      "  Training  | Loss:0.4045 \n",
      " Validation | Loss:0.4023 \n",
      "[Epoch 21 | 46976/47104] loss: 0.3328\n",
      "  Training  | Loss:0.4001 \n",
      " Validation | Loss:0.3969 \n",
      "[Epoch 22 | 46976/47104] loss: 0.2733\n",
      "  Training  | Loss:0.3948 \n",
      " Validation | Loss:0.3924 \n",
      "[Epoch 23 | 46976/47104] loss: 0.2709\n",
      "  Training  | Loss:0.3902 \n",
      " Validation | Loss:0.3876 \n",
      "[Epoch 24 | 46976/47104] loss: 0.3007\n",
      "  Training  | Loss:0.3853 \n",
      " Validation | Loss:0.3828 \n",
      "[Epoch 25 | 46976/47104] loss: 0.2849\n",
      "  Training  | Loss:0.3812 \n",
      " Validation | Loss:0.3794 \n",
      "[Epoch 26 | 46976/47104] loss: 0.2343\n",
      "  Training  | Loss:0.3779 \n",
      " Validation | Loss:0.3759 \n",
      "[Epoch 27 | 46976/47104] loss: 0.2134\n",
      "  Training  | Loss:0.3742 \n",
      " Validation | Loss:0.3718 \n",
      "[Epoch 28 | 46976/47104] loss: 0.3174\n",
      "  Training  | Loss:0.3700 \n",
      " Validation | Loss:0.3677 \n",
      "[Epoch 29 | 46976/47104] loss: 0.2792\n",
      "  Training  | Loss:0.3660 \n",
      " Validation | Loss:0.3641 \n",
      "[Epoch 30 | 46976/47104] loss: 0.2362\n",
      "  Training  | Loss:0.3626 \n",
      " Validation | Loss:0.3609 \n",
      "[Epoch 31 | 46976/47104] loss: 0.2879\n",
      "  Training  | Loss:0.3595 \n",
      " Validation | Loss:0.3577 \n",
      "[Epoch 32 | 46976/47104] loss: 0.2472\n",
      "  Training  | Loss:0.3558 \n",
      " Validation | Loss:0.3535 \n",
      "[Epoch 33 | 46976/47104] loss: 0.2610\n",
      "  Training  | Loss:0.3520 \n",
      " Validation | Loss:0.3499 \n"
     ]
    }
   ],
   "source": [
    "# start training! \n",
    "\n",
    "device = 'cuda:0'\n",
    "model = Net().to(device) \n",
    "model = train(train_dataloader, val_dataloader, model, n_epoch, batch, lr, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f183c",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "269463ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and predict testing data\n",
    "\n",
    "best_model = model\n",
    "best_model.load_state_dict(torch.load(\"model.pth\"))\n",
    "best_model = best_model.eval()\n",
    "\n",
    "testset = TestDataset()\n",
    "test_dataloader = DataLoader(testset, 1, False)\n",
    "result = []\n",
    "for x in test_dataloader:\n",
    "    x = x.to(device)\n",
    "    result.append(best_model(x).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bcdf616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate (key, probability) pairs\n",
    "\n",
    "keys_to_predict = sorted(public_x_alert_date['alert_key'].values.tolist())\n",
    "pairs = np.array(list(zip(keys_to_predict, result)))\n",
    "sorted_pairs = np.flip(pairs[pairs[:, 1].argsort()], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eafb3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output file \n",
    "\n",
    "example_keys = []\n",
    "with open('example.csv', newline='') as example:\n",
    "    rows = csv.reader(example)\n",
    "    headers = next(rows)\n",
    "    for row in rows:\n",
    "        example_keys.append(int(row[0]))\n",
    "        \n",
    "with open('predict.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['alert_key','probability'])\n",
    "    for row in sorted_pairs:\n",
    "        writer.writerow([int(row[0]), row[1]])\n",
    "    for key in example_keys:\n",
    "        if key not in keys_to_predict:\n",
    "            writer.writerow([key, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edf5c3",
   "metadata": {},
   "source": [
    "### Score\n",
    "In this part we load the answer of public testcase published by the competition just to calculate the score. The answer is not used to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e972491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.010351966873706004\n"
     ]
    }
   ],
   "source": [
    "ESun_public_y_answer = pd.read_csv('ESun_public_y_answer.csv')\n",
    "\n",
    "index_list = []\n",
    "SAR_count = 0\n",
    "for key, flag in ESun_public_y_answer.values.tolist():\n",
    "    if flag == 1:\n",
    "        SAR_count += 1\n",
    "        for idx in range(len(sorted_pairs)):\n",
    "            if key == sorted_pairs[idx][0]:\n",
    "                index_list.append(idx + 1)\n",
    "                break\n",
    "index_list.sort()\n",
    "print(\"score: \", str((SAR_count - 1) / index_list[-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a86d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b211dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ae2cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
