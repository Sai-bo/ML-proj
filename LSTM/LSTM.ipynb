{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "cbvN_zhuFCbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fhKK9nKMFApe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import packages."
      ],
      "metadata": {
        "id": "DD__8ohVBhjx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybs7iymbWbER"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the dataset and unzip it."
      ],
      "metadata": {
        "id": "6FrO0GquBkN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1vHDhZSPmhithLVRRzkNw0ak_kk7PhInu\n",
        "!gdown 145T8z3XXlsaISzWdrJgJVANGokG1XuFI\n",
        "!gdown 1uFCx21bqE3FnrdfvN_mwtw2-nvogTEex\n",
        "!gdown 1zZo9RLt3mMmJZxEETSY2g9ND31qkZIn0\n",
        "!gdown 1AubrUmeNUgpgiOu4tay6Gwl8O3lBaokF\n",
        "!gdown 1ZOXGT_rIdEGIliHGKEH3ha77ZlZyq1Gn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRjym0iYOoPN",
        "outputId": "8eea2d06-a582-4cb3-f09a-70b63f748a34"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1vHDhZSPmhithLVRRzkNw0ak_kk7PhInu \n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=145T8z3XXlsaISzWdrJgJVANGokG1XuFI\n",
            "To: /content/V_remit.csv\n",
            "100% 90.4M/90.4M [00:00<00:00, 118MB/s]\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1uFCx21bqE3FnrdfvN_mwtw2-nvogTEex \n",
            "\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zZo9RLt3mMmJZxEETSY2g9ND31qkZIn0\n",
            "To: /content/V_cred.csv\n",
            "100% 12.3M/12.3M [00:00<00:00, 179MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AubrUmeNUgpgiOu4tay6Gwl8O3lBaokF\n",
            "To: /content/V_info.csv\n",
            "100% 557k/557k [00:00<00:00, 159MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ZOXGT_rIdEGIliHGKEH3ha77ZlZyq1Gn\n",
            "To: /content/train_y.csv\n",
            "100% 204k/204k [00:00<00:00, 96.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic setup of hyperparameters"
      ],
      "metadata": {
        "id": "uNA6q0rnBwHP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VX5NXp4WbEi"
      },
      "source": [
        "'''\n",
        "BATCH_SIZE = 256\n",
        "EPOCH_NUM = 50\n",
        "MAX_POSITIONS_LEN = 100\n",
        "SEED = 97562246875 % (2**32-1) # Set your lucky number as the random seed\n",
        "MODEL_DIR = 'model.pth'\n",
        "lr = 5 * 1e-4\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "w2v_config = {'path': 'w2v.model', 'dim': 128}\n",
        "net_config = {'hidden_dim': 64, 'num_layers': 3, 'bidirectional': False, 'fix_embedding': True}\n",
        "header_config = {'dropout': 0.5, 'hidden_dim': 64}\n",
        "assert header_config['hidden_dim'] == net_config['hidden_dim'] or header_config['hidden_dim'] == net_config['hidden_dim'] * 2\n",
        "'''"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auxiliary functions and classes definition"
      ],
      "metadata": {
        "id": "FxOddxufB2dr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7PZMBJSWbEk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "9fcd1306-2e97-4a5e-dc75-601fedfafc9a"
      },
      "source": [
        "V_cred = pd.read_csv(\"V_cred.csv\", index_col=0) # 117: 13 months * 9 data\n",
        "V_info = pd.read_csv(\"V_info.csv\", index_col=0) # 4: risk_rank  occupation_code  total_asset  AGE             \n",
        "V_remit = pd.read_csv(\"V_remit.csv\", index_col=0) # 1572: 393 days * 4\n",
        "train_Y = pd.read_csv(\"train_y.csv\", index_col=0)\n",
        "\n",
        "#print(V_cred)\n",
        "#print(V_info)\n",
        "#print(V_remit)\n",
        "#print(train_X)\n",
        "#print(train_y)\n",
        "SAR_idx = []\n",
        "for i in range (len(train_Y)):\n",
        "    if (train_Y.iloc[i,0] == 1):\n",
        "        SAR_idx.append(i)\n",
        "print(len(SAR_idx), SAR_idx)\n",
        "\n",
        "'''\n",
        "# copy more data with SAR\n",
        "# Maybe we can do this\n",
        "for i in range (len(SAR_idx)):\n",
        "    if (i % 50 == 0):\n",
        "        print(i, \"/\", len(SAR_idx))\n",
        "    for j in range (30):\n",
        "        V_cred.loc[len(V_cred)] = V_remit.loc[SAR_idx[i]]\n",
        "print(V_cred)\n",
        "'''"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "234 [81, 181, 436, 1598, 1634, 1728, 1927, 2116, 2179, 2307, 2377, 2525, 2784, 2815, 2819, 2842, 2919, 2938, 2983, 3144, 3152, 3172, 3237, 3306, 3372, 3437, 3454, 3469, 3550, 3603, 3635, 3703, 3735, 3749, 3789, 3803, 3876, 3901, 3921, 3922, 3937, 4035, 4040, 4074, 4081, 4082, 4139, 4142, 4143, 4246, 4266, 4269, 4322, 4337, 4355, 4366, 4382, 4391, 4438, 4471, 4480, 4499, 4520, 4535, 4644, 4655, 4726, 4748, 4759, 4763, 4862, 4875, 4977, 5007, 5012, 5157, 5223, 5239, 5250, 5284, 5310, 5347, 5371, 5395, 5441, 5487, 5515, 5523, 5524, 5548, 5567, 5817, 5873, 5880, 5917, 5965, 6009, 6086, 6108, 6130, 6200, 6232, 6259, 6262, 6445, 6459, 6530, 6551, 6686, 6741, 6789, 6841, 6858, 6866, 6909, 6920, 6943, 6958, 6988, 7050, 7090, 7111, 7152, 7198, 7268, 7348, 7437, 7462, 7598, 7635, 7655, 7717, 7737, 7941, 7995, 8071, 8131, 8357, 8374, 8469, 8505, 8514, 8618, 8684, 8692, 8770, 8823, 9028, 9156, 9448, 9453, 9456, 9508, 9785, 9935, 10145, 10200, 10231, 10294, 10335, 10366, 10433, 10450, 10503, 10611, 10652, 10738, 10895, 11029, 11264, 11434, 11556, 11598, 11678, 11693, 11715, 11724, 11817, 11859, 11899, 11978, 12110, 12126, 12163, 12164, 12297, 12409, 12595, 12651, 12986, 13114, 13144, 13263, 13301, 13793, 13818, 13898, 14194, 14349, 14409, 14426, 14645, 14879, 14969, 15098, 15136, 15358, 15751, 16078, 16176, 16577, 16691, 17324, 17369, 17463, 17573, 17835, 17859, 17866, 17932, 18011, 18027, 18618, 18705, 18898, 19316, 20245, 22535, 22890, 23171, 23317, 23414, 23519, 23828]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# copy more data with SAR\\nfor i in range (len(SAR_idx)):\\n    if (i % 50 == 0):\\n        print(i, \"/\", len(SAR_idx))\\n    for j in range (REPEAT_NUMBERS):\\n        V_cred.loc[len(V_cred)] = V_remit.loc[SAR_idx[i]]\\n\\n\\nprint(V_cred)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.LSTM(units = 50, return_sequences = True, input_shape = (13,9)))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "#model.add(keras.layers.LSTM(units = 50, return_sequences = True))\n",
        "#model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.LSTM(units = 50))\n",
        "model.add(keras.layers.Dropout(0.2))\n",
        "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='nadam')\n",
        "\n",
        "\n",
        "tempx = np.array(V_cred)\n",
        "tempy = np.array(train_Y)\n",
        "print(tempx.shape)\n",
        "tempx = np.reshape(tempx, (tempx.shape[0], 13, 9))\n",
        "#tempy = np.reshape(tempy, (tempy.shape[0], tempy.shape[1], 1))\n",
        "print(tempx.shape)\n",
        "model.fit(tempx, tempy, epochs = 50, batch_size = 32) # epoch can be larger\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwjcyQq7VL7u",
        "outputId": "193efe16-a822-4207-f1c5-c47959f0bd63"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23906, 117)\n",
            "(23906, 13, 9)\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "748/748 [==============================] - 13s 12ms/step - loss: 0.0677\n",
            "Epoch 2/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0550\n",
            "Epoch 3/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0541\n",
            "Epoch 4/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0544\n",
            "Epoch 5/50\n",
            "748/748 [==============================] - 10s 14ms/step - loss: 0.0542\n",
            "Epoch 6/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0543\n",
            "Epoch 7/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0540\n",
            "Epoch 8/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0542\n",
            "Epoch 9/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0535\n",
            "Epoch 10/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0535\n",
            "Epoch 11/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0535\n",
            "Epoch 12/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0536\n",
            "Epoch 13/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0531\n",
            "Epoch 14/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0536\n",
            "Epoch 15/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0533\n",
            "Epoch 16/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0531\n",
            "Epoch 17/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0525\n",
            "Epoch 18/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0529\n",
            "Epoch 19/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0533\n",
            "Epoch 20/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0528\n",
            "Epoch 21/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0526\n",
            "Epoch 22/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0526\n",
            "Epoch 23/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0525\n",
            "Epoch 24/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0528\n",
            "Epoch 25/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0526\n",
            "Epoch 26/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0524\n",
            "Epoch 27/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0525\n",
            "Epoch 28/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0523\n",
            "Epoch 29/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0523\n",
            "Epoch 30/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0524\n",
            "Epoch 31/50\n",
            "748/748 [==============================] - 10s 14ms/step - loss: 0.0519\n",
            "Epoch 32/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0516\n",
            "Epoch 33/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0518\n",
            "Epoch 34/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0522\n",
            "Epoch 35/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0519\n",
            "Epoch 36/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0519\n",
            "Epoch 37/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0510\n",
            "Epoch 38/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0510\n",
            "Epoch 39/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0511\n",
            "Epoch 40/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0513\n",
            "Epoch 41/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0505\n",
            "Epoch 42/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0509\n",
            "Epoch 43/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0506\n",
            "Epoch 44/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0507\n",
            "Epoch 45/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0504\n",
            "Epoch 46/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0513\n",
            "Epoch 47/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0510\n",
            "Epoch 48/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0501\n",
            "Epoch 49/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0499\n",
            "Epoch 50/50\n",
            "748/748 [==============================] - 9s 12ms/step - loss: 0.0501\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0b834f5b50>"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = tempx\n",
        "#print(test)\n",
        "'''\n",
        "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
        "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
        "inputs = inputs.reshape(-1,1)\n",
        "inputs = sc.transform(inputs) # Feature Scaling\n",
        "'''\n",
        "prediction = model.predict(test)\n",
        "print(prediction, prediction.shape)\n",
        "result = pd.concat([pd.DataFrame(prediction, columns=[\"prob\"]), train_y], axis=1) \n",
        "result = result.sort_values(by = 'prob')\n",
        "print(result)\n",
        "for i in range (len(result)):\n",
        "    if (result.iloc[i,1] == 1):\n",
        "        print(\"The\", i, \"th lowest prediction has SAR with prob\", result.iloc[i,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeQcGAotjr3u",
        "outputId": "0948bab3-05bf-431c-dd08-28ed979d0ce6"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "748/748 [==============================] - 3s 4ms/step\n",
            "[[0.00922656]\n",
            " [0.00922656]\n",
            " [0.01416235]\n",
            " ...\n",
            " [0.01416235]\n",
            " [0.01416235]\n",
            " [0.00017959]] (23906, 1)\n",
            "           prob  0\n",
            "21558  0.000065  0\n",
            "20359  0.000065  0\n",
            "21421  0.000065  0\n",
            "18366  0.000065  0\n",
            "17445  0.000065  0\n",
            "...         ... ..\n",
            "7941   0.790488  1\n",
            "5880   0.814637  1\n",
            "3237   0.821980  1\n",
            "6909   0.827619  1\n",
            "3469   0.876941  1\n",
            "\n",
            "[23906 rows x 2 columns]\n",
            "The 4962 th lowest prediction has SAR with prob 0.0006045204\n",
            "The 6334 th lowest prediction has SAR with prob 0.0021804383\n",
            "The 7495 th lowest prediction has SAR with prob 0.0026288296\n",
            "The 7794 th lowest prediction has SAR with prob 0.002628831\n",
            "The 8197 th lowest prediction has SAR with prob 0.0028699054\n",
            "The 8365 th lowest prediction has SAR with prob 0.0030627719\n",
            "The 8720 th lowest prediction has SAR with prob 0.0036432114\n",
            "The 8868 th lowest prediction has SAR with prob 0.0046093445\n",
            "The 8900 th lowest prediction has SAR with prob 0.004807035\n",
            "The 8911 th lowest prediction has SAR with prob 0.0049158074\n",
            "The 9205 th lowest prediction has SAR with prob 0.006513517\n",
            "The 9282 th lowest prediction has SAR with prob 0.0066451686\n",
            "The 9379 th lowest prediction has SAR with prob 0.0066451686\n",
            "The 9434 th lowest prediction has SAR with prob 0.0066451686\n",
            "The 9601 th lowest prediction has SAR with prob 0.0067720474\n",
            "The 9604 th lowest prediction has SAR with prob 0.0068635824\n",
            "The 9738 th lowest prediction has SAR with prob 0.006867351\n",
            "The 9783 th lowest prediction has SAR with prob 0.0070602205\n",
            "The 9985 th lowest prediction has SAR with prob 0.007475591\n",
            "The 10294 th lowest prediction has SAR with prob 0.00855558\n",
            "The 10321 th lowest prediction has SAR with prob 0.008754512\n",
            "The 10400 th lowest prediction has SAR with prob 0.008977483\n",
            "The 10601 th lowest prediction has SAR with prob 0.009226562\n",
            "The 10701 th lowest prediction has SAR with prob 0.009226562\n",
            "The 10725 th lowest prediction has SAR with prob 0.009226562\n",
            "The 10858 th lowest prediction has SAR with prob 0.009226562\n",
            "The 10870 th lowest prediction has SAR with prob 0.009226562\n",
            "The 11123 th lowest prediction has SAR with prob 0.009226562\n",
            "The 11332 th lowest prediction has SAR with prob 0.009226562\n",
            "The 11343 th lowest prediction has SAR with prob 0.009226562\n",
            "The 11441 th lowest prediction has SAR with prob 0.009330186\n",
            "The 11475 th lowest prediction has SAR with prob 0.009350818\n",
            "The 11501 th lowest prediction has SAR with prob 0.009370495\n",
            "The 11614 th lowest prediction has SAR with prob 0.00938353\n",
            "The 11680 th lowest prediction has SAR with prob 0.009668458\n",
            "The 11743 th lowest prediction has SAR with prob 0.009820365\n",
            "The 11856 th lowest prediction has SAR with prob 0.00982878\n",
            "The 11896 th lowest prediction has SAR with prob 0.00982878\n",
            "The 12032 th lowest prediction has SAR with prob 0.00983768\n",
            "The 12056 th lowest prediction has SAR with prob 0.009839602\n",
            "The 12074 th lowest prediction has SAR with prob 0.0098508755\n",
            "The 12077 th lowest prediction has SAR with prob 0.009851024\n",
            "The 12298 th lowest prediction has SAR with prob 0.009996996\n",
            "The 12428 th lowest prediction has SAR with prob 0.010234194\n",
            "The 12510 th lowest prediction has SAR with prob 0.010302615\n",
            "The 12549 th lowest prediction has SAR with prob 0.010302615\n",
            "The 12635 th lowest prediction has SAR with prob 0.010302615\n",
            "The 12715 th lowest prediction has SAR with prob 0.010302615\n",
            "The 12874 th lowest prediction has SAR with prob 0.010344585\n",
            "The 13022 th lowest prediction has SAR with prob 0.010364694\n",
            "The 13071 th lowest prediction has SAR with prob 0.010364694\n",
            "The 13113 th lowest prediction has SAR with prob 0.010364694\n",
            "The 13210 th lowest prediction has SAR with prob 0.010364694\n",
            "The 13272 th lowest prediction has SAR with prob 0.010364694\n",
            "The 13363 th lowest prediction has SAR with prob 0.010381303\n",
            "The 13498 th lowest prediction has SAR with prob 0.010405387\n",
            "The 13572 th lowest prediction has SAR with prob 0.01041797\n",
            "The 13713 th lowest prediction has SAR with prob 0.010852809\n",
            "The 13918 th lowest prediction has SAR with prob 0.0111158\n",
            "The 13920 th lowest prediction has SAR with prob 0.0111158\n",
            "The 13944 th lowest prediction has SAR with prob 0.0111158\n",
            "The 13980 th lowest prediction has SAR with prob 0.0111158\n",
            "The 14021 th lowest prediction has SAR with prob 0.011115805\n",
            "The 14124 th lowest prediction has SAR with prob 0.011187148\n",
            "The 14125 th lowest prediction has SAR with prob 0.011187148\n",
            "The 14192 th lowest prediction has SAR with prob 0.011219412\n",
            "The 14248 th lowest prediction has SAR with prob 0.011219412\n",
            "The 14403 th lowest prediction has SAR with prob 0.0114638405\n",
            "The 14482 th lowest prediction has SAR with prob 0.011759891\n",
            "The 14535 th lowest prediction has SAR with prob 0.012853955\n",
            "The 14594 th lowest prediction has SAR with prob 0.013974517\n",
            "The 14638 th lowest prediction has SAR with prob 0.01416235\n",
            "The 14653 th lowest prediction has SAR with prob 0.01416235\n",
            "The 14829 th lowest prediction has SAR with prob 0.01416235\n",
            "The 14832 th lowest prediction has SAR with prob 0.01416235\n",
            "The 14890 th lowest prediction has SAR with prob 0.01416235\n",
            "The 14948 th lowest prediction has SAR with prob 0.01416235\n",
            "The 14981 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15001 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15015 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15027 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15036 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15067 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15260 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15332 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15412 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15469 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15492 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15567 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15619 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15751 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15780 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15799 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15850 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15858 th lowest prediction has SAR with prob 0.01416235\n",
            "The 15990 th lowest prediction has SAR with prob 0.01416235\n",
            "The 16272 th lowest prediction has SAR with prob 0.01416235\n",
            "The 16332 th lowest prediction has SAR with prob 0.01416235\n",
            "The 16384 th lowest prediction has SAR with prob 0.01416235\n",
            "The 16712 th lowest prediction has SAR with prob 0.01416235\n",
            "The 17401 th lowest prediction has SAR with prob 0.01416235\n",
            "The 17528 th lowest prediction has SAR with prob 0.01416235\n",
            "The 17560 th lowest prediction has SAR with prob 0.01416235\n",
            "The 17594 th lowest prediction has SAR with prob 0.01416235\n",
            "The 17820 th lowest prediction has SAR with prob 0.01416235\n",
            "The 17849 th lowest prediction has SAR with prob 0.01416235\n",
            "The 17870 th lowest prediction has SAR with prob 0.01416235\n",
            "The 17984 th lowest prediction has SAR with prob 0.01416235\n",
            "The 18136 th lowest prediction has SAR with prob 0.01416235\n",
            "The 18629 th lowest prediction has SAR with prob 0.01416235\n",
            "The 18823 th lowest prediction has SAR with prob 0.01416235\n",
            "The 18837 th lowest prediction has SAR with prob 0.01416235\n",
            "The 18906 th lowest prediction has SAR with prob 0.01416235\n",
            "The 18919 th lowest prediction has SAR with prob 0.01416235\n",
            "The 18927 th lowest prediction has SAR with prob 0.01416235\n",
            "The 18962 th lowest prediction has SAR with prob 0.01416235\n",
            "The 18999 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19010 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19023 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19052 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19168 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19203 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19230 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19428 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19447 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19462 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19483 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19492 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19541 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19553 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19661 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19680 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19695 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19762 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19767 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19798 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19854 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19892 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19950 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19970 th lowest prediction has SAR with prob 0.01416235\n",
            "The 19971 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20048 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20127 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20233 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20372 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20394 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20398 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20402 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20525 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20535 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20590 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20599 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20705 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20842 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20845 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20865 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20933 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20953 th lowest prediction has SAR with prob 0.01416235\n",
            "The 20992 th lowest prediction has SAR with prob 0.01416235\n",
            "The 21254 th lowest prediction has SAR with prob 0.01416235\n",
            "The 21328 th lowest prediction has SAR with prob 0.01416235\n",
            "The 21512 th lowest prediction has SAR with prob 0.01416235\n",
            "The 21615 th lowest prediction has SAR with prob 0.01416235\n",
            "The 21622 th lowest prediction has SAR with prob 0.01416235\n",
            "The 21752 th lowest prediction has SAR with prob 0.01416235\n",
            "The 21774 th lowest prediction has SAR with prob 0.01416235\n",
            "The 21819 th lowest prediction has SAR with prob 0.01416235\n",
            "The 21823 th lowest prediction has SAR with prob 0.01416235\n",
            "The 21900 th lowest prediction has SAR with prob 0.015764395\n",
            "The 21926 th lowest prediction has SAR with prob 0.015951995\n",
            "The 21973 th lowest prediction has SAR with prob 0.016747994\n",
            "The 22043 th lowest prediction has SAR with prob 0.016747994\n",
            "The 22084 th lowest prediction has SAR with prob 0.016747994\n",
            "The 22150 th lowest prediction has SAR with prob 0.016747994\n",
            "The 22160 th lowest prediction has SAR with prob 0.016747994\n",
            "The 22202 th lowest prediction has SAR with prob 0.016747994\n",
            "The 22275 th lowest prediction has SAR with prob 0.016953597\n",
            "The 22281 th lowest prediction has SAR with prob 0.016988678\n",
            "The 22537 th lowest prediction has SAR with prob 0.017839573\n",
            "The 22630 th lowest prediction has SAR with prob 0.018643172\n",
            "The 22658 th lowest prediction has SAR with prob 0.018669046\n",
            "The 22760 th lowest prediction has SAR with prob 0.019449653\n",
            "The 22802 th lowest prediction has SAR with prob 0.02077082\n",
            "The 22809 th lowest prediction has SAR with prob 0.021956516\n",
            "The 22849 th lowest prediction has SAR with prob 0.02633571\n",
            "The 22892 th lowest prediction has SAR with prob 0.028440751\n",
            "The 22958 th lowest prediction has SAR with prob 0.03868396\n",
            "The 23014 th lowest prediction has SAR with prob 0.043415517\n",
            "The 23018 th lowest prediction has SAR with prob 0.043416608\n",
            "The 23057 th lowest prediction has SAR with prob 0.049238898\n",
            "The 23059 th lowest prediction has SAR with prob 0.04946996\n",
            "The 23063 th lowest prediction has SAR with prob 0.04946996\n",
            "The 23121 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23127 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23155 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23163 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23179 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23200 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23226 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23247 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23261 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23279 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23285 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23300 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23351 th lowest prediction has SAR with prob 0.04946998\n",
            "The 23482 th lowest prediction has SAR with prob 0.050592925\n",
            "The 23495 th lowest prediction has SAR with prob 0.050596863\n",
            "The 23554 th lowest prediction has SAR with prob 0.050597325\n",
            "The 23596 th lowest prediction has SAR with prob 0.050597325\n",
            "The 23601 th lowest prediction has SAR with prob 0.050597325\n",
            "The 23628 th lowest prediction has SAR with prob 0.050597325\n",
            "The 23667 th lowest prediction has SAR with prob 0.050597325\n",
            "The 23672 th lowest prediction has SAR with prob 0.050597325\n",
            "The 23734 th lowest prediction has SAR with prob 0.05137083\n",
            "The 23751 th lowest prediction has SAR with prob 0.05137083\n",
            "The 23771 th lowest prediction has SAR with prob 0.05137083\n",
            "The 23789 th lowest prediction has SAR with prob 0.052180532\n",
            "The 23827 th lowest prediction has SAR with prob 0.060198884\n",
            "The 23847 th lowest prediction has SAR with prob 0.06758866\n",
            "The 23857 th lowest prediction has SAR with prob 0.07729428\n",
            "The 23874 th lowest prediction has SAR with prob 0.12570585\n",
            "The 23885 th lowest prediction has SAR with prob 0.22966388\n",
            "The 23887 th lowest prediction has SAR with prob 0.31793073\n",
            "The 23892 th lowest prediction has SAR with prob 0.38876176\n",
            "The 23896 th lowest prediction has SAR with prob 0.6482362\n",
            "The 23897 th lowest prediction has SAR with prob 0.6874913\n",
            "The 23898 th lowest prediction has SAR with prob 0.77474165\n",
            "The 23899 th lowest prediction has SAR with prob 0.7852002\n",
            "The 23900 th lowest prediction has SAR with prob 0.789092\n",
            "The 23901 th lowest prediction has SAR with prob 0.7904879\n",
            "The 23902 th lowest prediction has SAR with prob 0.81463724\n",
            "The 23903 th lowest prediction has SAR with prob 0.82198\n",
            "The 23904 th lowest prediction has SAR with prob 0.8276186\n",
            "The 23905 th lowest prediction has SAR with prob 0.8769411\n"
          ]
        }
      ]
    }
  ]
}